This list is to provide a record of problems for tests to add coverage for if possible.

does not always clear running instances, leading to it rejecting offers when it needs to heal
  fixed

does not retrieve persisted framework id when clusterName = t3
  fixed

does not deconfigure node before trying to add a new one, livelocking the cluster
  cause: was trying to deconfigure a nonexistent node id
  fixed

when fw dies, then slave dies, then fw fails over, it will launch before removing the slave that died while the fw was down
  cause: fw did not verify that defunct nodes are purged before launching
  fixed

when all tasks are killed, scheduler blocks indefinitely while trying to deconfigure them
  should: verify that all etcd instances are unavailable, initiate HARD RESET (to be called by register and reregister as well)
  fixed - but should we block instead and await operator intervention?

with chillfactor of 1, it's possible to spawn more than 1 instance per slave
  fixed

json serialization was overriden
  fixed

deadlock in offercache due to lock acquisition in a for loop
  fixed

scheduler does not bail out if the configured executor or etcd binaries are not present
  fixed

if multiple tasks are launched concurrently they will have split configurations
  fixed

when executor comes up, it fails to configure but it still thinks it's ok:
  fixed - executor now bails out instantly if it can't reach the first node it reaches out to for initial configuration

when reseeding, it never kills the inferior nodes
  fixed - was pulling from incomplete list of live candidates instead of all running (possibly unhealthy) tasks

tasks in pending are not considered when enforcing node-uniqueness
  fixed - was not taking new "TASK_STARTING" nodes into account, and TASK_STARTING would clear from pending

tasks possibly deconfigured incorrectly?  or they think they are during reseed?
  fixed - need to run etcdctl backup on data dir first to strip out old membership

when starting on executor, it can crash if it starts with a previously-used name without the wal being present
  possibly related to above issue? waiting for resurfacing

executors stay alive forever when master, slave, or etcd instance fails

executors in staging state for a long time (don't send any updates) will just stay alive as zombies forever, consuming offers

Prune sees a starting node in the member list with the name "" (empty string), doesn't recognize it, and tries to kill it
  nodes were not successfully joining a reseedeg cluster due to failure to strip previous cluster metadata during reseed
